{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Dot\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_limpio = pd.read_csv(\"train_limpio_con_BOW_de_5000_y_Stemming_noDrops.csv\",encoding = \"ISO-8859-1\")\n",
    "test_limpio = pd.read_csv(\"test_limpio_con_BOW_de_5000_y_Stemming_noDrops.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    our deed are the reason of this earthquak may ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_limpio[\"text\"].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our', 'deed', 'are', 'the', 'reason', 'of', 'this']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def addWordsToList(wordsList, string):\n",
    "    for word in string.split(\" \"):\n",
    "        wordsList.append(word)\n",
    "\n",
    "words = []\n",
    "train_limpio.loc[train_limpio.text.notnull()][\"text\"].apply(lambda x: addWordsToList(words, x))\n",
    "words[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = .8\n",
    "TEST_SIZE = .9\n",
    "df = train_limpio\n",
    "raw_train_df, raw_val_df, raw_test_df = np.split(df.sample(frac=1), [int(VALIDATION_SIZE*len(df)), int(TEST_SIZE*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_df_no_target = raw_train_df[[\"text\"]]\n",
    "raw_val_df_no_target = raw_val_df[[\"text\"]]\n",
    "raw_test_df_no_target = raw_test_df[[\"text\"]]\n",
    "\n",
    "raw_train_df_target = raw_train_df[[\"target\"]]\n",
    "raw_val_df_target = raw_val_df[[\"target\"]]\n",
    "raw_test_df_target = raw_test_df[[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con esta funcion representamos cada palabra con un int unico y devolvemos una lista con las más comunes\n",
    "#Un dic con la cantidad de cada una.\n",
    "\n",
    "#Extract the top 10,000 most common words to include in our embedding vector\n",
    "#Gather together all the unique words and index them with a unique integer value – this is what is required to create an equivalent one-hot type input for the word.  We’ll use a dictionary to do this\n",
    "#Loop through every word in the dataset (vocabulary variable) and assign it to the unique integer word identified, created in Step 2 above.  This will allow easy lookup / processing of the word data stream\n",
    "vocab_size = 10000\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "vector_dim = 300\n",
    "epochs = 1000000\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution. Son las más comunes.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6489, 2615], [1778, 1614], [184, 20], [7939, 4448], [9643, 5063], [7682, 2730], [5303, 167], [5789, 5566], [5075, 6244], [1051, 7519]] [0, 1, 1, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(list(raw_train_df_no_target.to_numpy(dtype=\"float32\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_target = keras.Input((1,))\n",
    "input_context = keras.Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "similarity = Dot(1, normalize=True)([target, context])\n",
    "\n",
    "dot_product = Dot(1)([target, context])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "model = Model([input_target, input_context], output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "validation_model = Model([input_target, input_context], similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        for i in range(vocab_size):\n",
    "            in_arr1[0,] = valid_word_idx\n",
    "            in_arr2[0,] = i\n",
    "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "sim_cb = SimilarityCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.6939670443534851\n",
      "Nearest to go: bokoharam, broken, wbre, elbestia, orchestr, Ã£Â¥, crane, mariomaraczi,\n",
      "Nearest to one: milkshak, obsolet, kit, yolk, idi, bowknot, farrakhan, zenandemcfen,\n",
      "Nearest to famili: window, pan, usnwsgov, vita, dampen, mackayim, boatnew, hereÃ¢ÂÃ£ÂÃ¢,\n",
      "Nearest to as: steep, eg, underground, theblaz, friggin, strain, marginoferror, Ã£Â¥,\n",
      "Nearest to how: club, Ã£ÂÃ£Â, whirlwind, eri, deck, probe, raung, tree,\n",
      "Nearest to after: bradley, christycroley, papcrdol, haiyan, director, doctorfluxx, Ã¢ÂÃ£ÂÃ£Â·hoax, worldwid,\n",
      "Nearest to over: celticinde, drain, present, cnv, xmen, spec, drink, welfar,\n",
      "Nearest to or: inbetween, thatÃ¢ÂÃ£ÂÃ¢, bethlehem, realest, diss, offend, lez, preview,\n",
      "Nearest to than: warcraft, recoveri, chart, postbattl, forecast, imagin, hop, nonexist,\n",
      "Nearest to via: shira, drothvad, iphooey, procedur, cam, season, vagersedolla, runner,\n",
      "Nearest to at: thereof, vincent, eastward, mpc, fiona, matthew, constel, awesomelov,\n",
      "Nearest to they: store, dental, atchisonsean, careen, page, devil, golfbal, bloodi,\n",
      "Nearest to into: precisionist, prophet, fotofil, godslov, patna, canberra, gtgtgt, roundhous,\n",
      "Nearest to bomb: brown, conclud, righteous, aggress, kick, sleepjunki, review, dissert,\n",
      "Nearest to of: tellyfckngo, die, bind, hushh, counter, case, tesco, dip,\n",
      "Nearest to was: offens, ortiz, jen, somehow, mullah, yolk, offr, test,\n",
      "Iteration 100, loss=0.6952593326568604\n",
      "Iteration 200, loss=0.6939767599105835\n",
      "Iteration 300, loss=0.6904749870300293\n",
      "Iteration 400, loss=0.6901012063026428\n",
      "Iteration 500, loss=0.6990745663642883\n",
      "Iteration 600, loss=0.7005290389060974\n",
      "Iteration 700, loss=0.7044674754142761\n",
      "Iteration 800, loss=0.6786973476409912\n",
      "Iteration 900, loss=0.6952345371246338\n",
      "Iteration 1000, loss=0.6942052245140076\n",
      "Iteration 1100, loss=0.6928874850273132\n",
      "Iteration 1200, loss=0.6919259428977966\n",
      "Iteration 1300, loss=0.6931204795837402\n",
      "Iteration 1400, loss=0.7077330350875854\n",
      "Iteration 1500, loss=0.6783053278923035\n",
      "Iteration 1600, loss=0.7114395499229431\n",
      "Iteration 1700, loss=0.6811063885688782\n",
      "Iteration 1800, loss=0.6764946579933167\n",
      "Iteration 1900, loss=0.716042697429657\n",
      "Iteration 2000, loss=0.7276505827903748\n",
      "Iteration 2100, loss=0.7239091992378235\n",
      "Iteration 2200, loss=0.6558012962341309\n"
     ]
    }
   ],
   "source": [
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "    if cnt % 10000 == 0:\n",
    "        sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1.evaluate(raw_test_df_no_target.to_numpy(dtype=\"uint\"), raw_test_df_target.to_numpy(dtype=\"uint\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
