{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from tensorflow.python.keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('train_limpio_sin_vect_textos.csv')\n",
    "train_tfidf = pd.read_csv('train_TfIdf_Dimensiones_Reducidads.csv')\n",
    "test_df=pd.read_csv('test_limpio_sin_vect_textos.csv')\n",
    "test_tfidf = pd.read_csv('test_TfIdf_Dimensiones_Reducidas.csv')\n",
    "sample = pd.read_csv(\"sample_submission.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower()\n",
    "       \n",
    "    ## Remove stop words (mejor las dejo)\n",
    "    #text = text.split()\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "    #text = [w for w in text if not w in stops and len(w) >= 3]  \n",
    "    #text = \" \".join(text)\n",
    "\n",
    "    ## Clean the text\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  #quito caracteres inusuales\n",
    "    text = re.sub('â' , '', text)  #quito caracteres inusuales\n",
    "    text = re.sub('ª' , '', text)  #quito caracteres inusuales\n",
    "    text = re.sub('ã' , '', text)  #quito caracteres inusuales\n",
    "    text = re.sub('¼' , '', text)  #quito caracteres inusuales\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)  # quito numeros\n",
    "    text = re.sub(\"[^0-9a-zA-Z]+\", \" \", text)\n",
    "    \n",
    "    ## Stemming\n",
    "    #text = text.split()\n",
    "    #stemmer = SnowballStemmer('english')\n",
    "    #stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    #text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].replace(r'http\\S+', '', regex=True)\n",
    "test_df['text'] = test_df['text'].replace(r'http\\S+', '', regex=True)\n",
    "train_df['text'] = train_df['text'].apply(lambda x: str(x).lower())\n",
    "test_df['text'] = test_df['text'].apply(lambda x: str(x).lower())\n",
    "target = train_df.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparo el texto para el embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto no deberia ser necesario\n",
    "train_df.text=train_df.text.map(lambda x: clean_text(x))\n",
    "test_df.text=test_df.text.map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "train_data = pad_sequences(train_sequences, maxlen=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "test_data = pad_sequences(test_sequences, maxlen=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'glove-dataset/glove.twitter.27B.200d.txt' \n",
    "#Por ahora lo dejoe n 50, supongo que solo mejora con 200 si complejizo la red nueronal\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((50000, 200)) #50 o 200, segun las dimensiones del objeto que use\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > 50000 - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparo el input de las keywords para la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['keyword'] = train_df['keyword'].apply(lambda x: str(x).lower())\n",
    "test_df['keyword'] = test_df['keyword'].apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['keyword'])\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['keyword'])\n",
    "train_keyword_data = pad_sequences(train_sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_df['keyword'])\n",
    "test_keyword_data = pad_sequences(test_sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparo el input de las demas features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_text = train_df.drop(columns=['text','keyword','target']).join(train_tfidf)\n",
    "test_no_text = test_df.drop(columns=['text','keyword']).join(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agrego las otras predicciones como features\n",
    "pred_train_catboost=pd.read_csv('feature_pred/prediccion_train_catBoost.csv')\n",
    "pred_test_catboost=pd.read_csv('feature_pred/prediccion_test_catBoost.csv')\n",
    "pred_train_bayes=pd.read_csv('feature_pred/prediccion_train_bayes.csv')\n",
    "pred_test_bayes=pd.read_csv('feature_pred/prediccion_test_bayes.csv')\n",
    "pred_train_svm=pd.read_csv('feature_pred/prediccion_train_SVM.csv')\n",
    "pred_test_svm=pd.read_csv('feature_pred/prediccion_test_SVM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_text['pred_catbooost'] = pred_train_catboost.target\n",
    "test_no_text['pred_catbooost'] = pred_test_catboost.target\n",
    "\n",
    "train_no_text['pred_bayes'] = pred_train_bayes.target\n",
    "test_no_text['pred_bayes'] = pred_test_bayes.target\n",
    "\n",
    "train_no_text['pred_svm'] = pred_train_svm.target\n",
    "test_no_text['pred_svm'] = pred_test_svm.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainContinuous = cs.fit_transform(train_no_text)\n",
    "testContinuous = cs.fit_transform(test_no_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero pruebo sin keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=5, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits = 5)\n",
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_text_input = keras.Input(shape=(180,), name='nlp_text_input')\n",
    "meta_input = keras.Input(shape=(train_no_text.shape[1],), name='meta_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "153/153 - 63s - loss: 0.5092 - accuracy: 0.7634 - val_loss: 0.4204 - val_accuracy: 0.8168\n",
      "Epoch 2/5\n",
      "153/153 - 67s - loss: 0.4498 - accuracy: 0.7970 - val_loss: 0.4222 - val_accuracy: 0.8116\n",
      "Epoch 3/5\n",
      "153/153 - 60s - loss: 0.4267 - accuracy: 0.8082 - val_loss: 0.4214 - val_accuracy: 0.8148\n",
      "Epoch 4/5\n",
      "153/153 - 61s - loss: 0.4102 - accuracy: 0.8135 - val_loss: 0.4254 - val_accuracy: 0.8175\n",
      "Epoch 5/5\n",
      "153/153 - 59s - loss: 0.4001 - accuracy: 0.8232 - val_loss: 0.4263 - val_accuracy: 0.8148\n",
      "Epoch 1/5\n",
      "153/153 - 61s - loss: 0.5094 - accuracy: 0.7562 - val_loss: 0.4672 - val_accuracy: 0.7892\n",
      "Epoch 2/5\n",
      "153/153 - 64s - loss: 0.4303 - accuracy: 0.8066 - val_loss: 0.4699 - val_accuracy: 0.7912\n",
      "Epoch 3/5\n",
      "153/153 - 56s - loss: 0.4152 - accuracy: 0.8123 - val_loss: 0.4677 - val_accuracy: 0.7886\n",
      "Epoch 4/5\n",
      "153/153 - 56s - loss: 0.4076 - accuracy: 0.8163 - val_loss: 0.4655 - val_accuracy: 0.7899\n",
      "Epoch 5/5\n",
      "153/153 - 52s - loss: 0.3897 - accuracy: 0.8307 - val_loss: 0.4980 - val_accuracy: 0.7695\n",
      "Epoch 1/5\n",
      "153/153 - 53s - loss: 0.4930 - accuracy: 0.7762 - val_loss: 0.4936 - val_accuracy: 0.7794\n",
      "Epoch 2/5\n",
      "153/153 - 53s - loss: 0.4298 - accuracy: 0.8117 - val_loss: 0.4736 - val_accuracy: 0.7899\n",
      "Epoch 3/5\n",
      "153/153 - 61s - loss: 0.4045 - accuracy: 0.8205 - val_loss: 0.4904 - val_accuracy: 0.7538\n",
      "Epoch 4/5\n",
      "153/153 - 61s - loss: 0.3956 - accuracy: 0.8276 - val_loss: 0.4746 - val_accuracy: 0.7853\n",
      "Epoch 5/5\n",
      "153/153 - 60s - loss: 0.3818 - accuracy: 0.8305 - val_loss: 0.4680 - val_accuracy: 0.7846\n",
      "Epoch 1/5\n",
      "153/153 - 58s - loss: 0.5033 - accuracy: 0.7669 - val_loss: 0.4900 - val_accuracy: 0.7937\n",
      "Epoch 2/5\n",
      "153/153 - 60s - loss: 0.4321 - accuracy: 0.8079 - val_loss: 0.5024 - val_accuracy: 0.7760\n",
      "Epoch 3/5\n",
      "153/153 - 61s - loss: 0.4195 - accuracy: 0.8104 - val_loss: 0.4790 - val_accuracy: 0.8068\n",
      "Epoch 4/5\n",
      "153/153 - 62s - loss: 0.4049 - accuracy: 0.8192 - val_loss: 0.4529 - val_accuracy: 0.8016\n",
      "Epoch 5/5\n",
      "153/153 - 61s - loss: 0.3941 - accuracy: 0.8276 - val_loss: 0.4664 - val_accuracy: 0.7812\n",
      "Epoch 1/5\n",
      "153/153 - 62s - loss: 0.5041 - accuracy: 0.7544 - val_loss: 0.4334 - val_accuracy: 0.8081\n",
      "Epoch 2/5\n",
      "153/153 - 62s - loss: 0.4407 - accuracy: 0.8051 - val_loss: 0.4209 - val_accuracy: 0.8134\n",
      "Epoch 3/5\n",
      "153/153 - 57s - loss: 0.4297 - accuracy: 0.8073 - val_loss: 0.4184 - val_accuracy: 0.8127\n",
      "Epoch 4/5\n",
      "153/153 - 54s - loss: 0.4117 - accuracy: 0.8163 - val_loss: 0.4015 - val_accuracy: 0.8167\n",
      "Epoch 5/5\n",
      "153/153 - 59s - loss: 0.3965 - accuracy: 0.8209 - val_loss: 0.4079 - val_accuracy: 0.8127\n",
      "Error de test: [0.7967800393838027, 0.753234974078072, 0.7872224579729069, 0.7694231880381628, 0.817167913285478]\n"
     ]
    }
   ],
   "source": [
    "test_error=[]\n",
    "#Si agregamdp una dense mejora el resultado\n",
    "for train_index,test_index in kf.split(train_data):\n",
    "    x_train_words,x_test_words = train_data[train_index], train_data[test_index]\n",
    "    x_train_numerical,x_test_numerical = trainContinuous[train_index], trainContinuous[test_index]\n",
    "    y_train,y_test = train_df['target'][train_index], train_df['target'][test_index]\n",
    "    \n",
    "    emb = Embedding(50000, 200, input_length=100, weights=[embedding_matrix], trainable=False)(nlp_text_input)\n",
    "    nlp_text_out = Bidirectional(LSTM(200, dropout=0.6, recurrent_dropout=0.2))(emb)  #conv antes era emb\n",
    "\n",
    "\n",
    "    x = concatenate([nlp_text_out ,meta_input])\n",
    "    x = Dense(24, activation='relu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[nlp_text_input , meta_input], outputs=[x])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit([x_train_words, x_train_numerical], y_train,validation_data=([x_test_words,x_test_numerical],y_test), batch_size=40, epochs=5,verbose=2)\n",
    "\n",
    "    predictions= model.predict([x_test_words,x_test_numerical])\n",
    "\n",
    "    bin_pred = []\n",
    "    for i in predictions:\n",
    "        if (i<0.46):\n",
    "            bin_pred.append(0)\n",
    "        else:\n",
    "            bin_pred.append(1)\n",
    "\n",
    "    test_error.append(f1_score(y_test, bin_pred, average='macro'))\n",
    "    \n",
    "print('Error de test:',test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= model.predict([test_data,testContinuous])\n",
    "bin_pred = []\n",
    "bin_probably_wrong = []\n",
    "for i in predictions:\n",
    "    if (i<0.46): #Important\n",
    "        bin_pred.append(0)\n",
    "    else:\n",
    "        bin_pred.append(1)\n",
    "        \n",
    "for j in predictions:\n",
    "    if (j<0.55 and j > 0.45):\n",
    "        bin_probably_wrong.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.target = bin_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2060\n",
       "1    1203\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('NN-tfidf-predictions-5epochs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([x_test_words,x_test_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_train_spliteada = train_no_text.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_train_spliteada['tensor_flow_pred'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_train_spliteada['target'] = train_df['target'][test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_train_spliteada.to_csv('split_train_pred_tensor_flow.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= model.predict([test_data,testContinuous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_no_text['tensor_flow_pred'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_no_text[['tensor_flow_pred']].to_csv('test_pred_tensor_flow.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
