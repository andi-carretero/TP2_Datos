{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"sample_submission.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower()\n",
    "       \n",
    "    ## Remove stop words (mejor las dejo)\n",
    "    #text = text.split()\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "    #text = [w for w in text if not w in stops and len(w) >= 3]  \n",
    "    #text = \" \".join(text)\n",
    "\n",
    "    ## Clean the text\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  #quito caracteres inusuales\n",
    "    text = re.sub('â' , '', text)  #quito caracteres inusuales\n",
    "    text = re.sub('ª' , '', text)  #quito caracteres inusuales\n",
    "    text = re.sub('ã' , '', text)  #quito caracteres inusuales\n",
    "    text = re.sub('¼' , '', text)  #quito caracteres inusuales\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)  # quito numeros\n",
    "    text = re.sub(\"[^0-9a-zA-Z]+\", \" \", text)\n",
    "    \n",
    "    ## Stemming\n",
    "    #text = text.split()\n",
    "    #stemmer = SnowballStemmer('english')\n",
    "    #stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    #text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].replace(r'http\\S+', '', regex=True)\n",
    "test_df['text'] = test_df['text'].replace(r'http\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.text=train_df.text.map(lambda x: clean_text(x))\n",
    "test_df.text=test_df.text.map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "train_data = pad_sequences(train_sequences, maxlen=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "test_data = pad_sequences(test_sequences, maxlen=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'glove-dataset/glove.twitter.27B.200d.txt' \n",
    "#Por ahora lo dejoe n 50, supongo que solo mejora con 200 si complejizo la red nueronal\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((50000, 200)) #50 o 200, segun las dimensiones del objeto que use\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > 50000 - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(50000, 200, input_length=180, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(300))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separo train y test\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data, train_df['target'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "191/191 [==============================] - 15s 78ms/step - loss: 0.4940 - accuracy: 0.7747\n",
      "Epoch 2/2\n",
      "191/191 [==============================] - 14s 71ms/step - loss: 0.4034 - accuracy: 0.8279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f621c4d0c88>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove.fit(x_train, y_train, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-2066755a9fcc>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "pred = model_glove.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7814285973804204"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Calculo con k-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=5, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits = 5)\n",
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Igual esta \"mal\", deberia vectorizar y entrenar solo los valores del train (creo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "305/305 [==============================] - 16s 53ms/step - loss: 0.4870 - accuracy: 0.7703\n",
      "Epoch 2/2\n",
      "305/305 [==============================] - 16s 54ms/step - loss: 0.4067 - accuracy: 0.8243\n",
      "Epoch 1/2\n",
      "305/305 [==============================] - 17s 55ms/step - loss: 0.4780 - accuracy: 0.7849\n",
      "Epoch 2/2\n",
      "305/305 [==============================] - 17s 55ms/step - loss: 0.3968 - accuracy: 0.8261\n",
      "Epoch 1/2\n",
      "305/305 [==============================] - 18s 59ms/step - loss: 0.4705 - accuracy: 0.7783\n",
      "Epoch 2/2\n",
      "305/305 [==============================] - 18s 59ms/step - loss: 0.3943 - accuracy: 0.8297\n",
      "Epoch 1/2\n",
      "305/305 [==============================] - 17s 57ms/step - loss: 0.4841 - accuracy: 0.7721\n",
      "Epoch 2/2\n",
      "305/305 [==============================] - 17s 55ms/step - loss: 0.4017 - accuracy: 0.8258\n",
      "Epoch 1/2\n",
      "305/305 [==============================] - 17s 55ms/step - loss: 0.4833 - accuracy: 0.7747\n",
      "Epoch 2/2\n",
      "305/305 [==============================] - 17s 56ms/step - loss: 0.4093 - accuracy: 0.8238\n",
      "Error de test: [0.790783461991978, 0.7871848755751516, 0.7664144112021876, 0.768935714564012, 0.8113241175160086]\n"
     ]
    }
   ],
   "source": [
    "test_error=[]\n",
    "#Si agregamdp una dense mejora el resultado\n",
    "for train_index,test_index in kf.split(train_data):\n",
    "    x_train,x_test = train_data[train_index], train_data[test_index]\n",
    "    y_train,y_test = train_df['target'][train_index], train_df['target'][test_index]\n",
    "    \n",
    "    model_glove = Sequential()\n",
    "    model_glove.add(Embedding(50000, 200, input_length=180, weights=[embedding_matrix], trainable=False))\n",
    "    model_glove.add(Dropout(0.2))\n",
    "    model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "    model_glove.add(MaxPooling1D(pool_size=4))\n",
    "    model_glove.add(LSTM(300))\n",
    "    model_glove.add(Dense(1, activation='sigmoid'))\n",
    "    model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model_glove.fit(x_train,y_train, epochs=2, batch_size=20)\n",
    "    \n",
    "    pred = model_glove.predict_classes(x_test)\n",
    "\n",
    "    test_error.append(f1_score(y_test, pred, average='macro'))\n",
    "    \n",
    "print('Error de test:',test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizo la prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo el modelo devuelta\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(50000, 200, input_length=180, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(300))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "238/238 [==============================] - 17s 72ms/step - loss: 0.4830 - accuracy: 0.7795\n",
      "Epoch 2/2\n",
      "238/238 [==============================] - 17s 72ms/step - loss: 0.4041 - accuracy: 0.8181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6208042668>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove.fit(train_data, train_df['target'], epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text'] = test_df['text'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "test_data = pad_sequences(test_sequences, maxlen=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= model_glove.predict_classes(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.target = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('neural_network_glove_2_epochs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2134\n",
       "1    1129\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intento agregarle multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cs = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_input_2 = pd.read_csv(\"train_limpio_input_2_NN.csv\",encoding = \"ISO-8859-1\")\n",
    "test_input_2 = pd.read_csv(\"test_limpio_input_2_NN.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Le saco el target \n",
    "test_ids = test_input_2.pop(\"id\")\n",
    "target = train_input_2.target\n",
    "train_input_2 = train_input_2.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_input_2 = test_input_2.drop(columns=['text'])\n",
    "train_input_2 = train_input_2.drop(columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainContinuous = cs.fit_transform(train_input_2)\n",
    "testContinuous = cs.fit_transform(test_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 2850)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainContinuous.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contolo con k-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_input = keras.Input(shape=(180,), name='nlp_input')\n",
    "meta_input = keras.Input(shape=(2850,), name='meta_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "48/48 [==============================] - 15s 313ms/step - loss: 0.5197 - accuracy: 0.7521\n",
      "Epoch 2/7\n",
      "48/48 [==============================] - 15s 317ms/step - loss: 0.4174 - accuracy: 0.8164\n",
      "Epoch 3/7\n",
      "48/48 [==============================] - 15s 322ms/step - loss: 0.3924 - accuracy: 0.8299\n",
      "Epoch 4/7\n",
      "48/48 [==============================] - 15s 320ms/step - loss: 0.3720 - accuracy: 0.8429\n",
      "Epoch 5/7\n",
      "48/48 [==============================] - 16s 329ms/step - loss: 0.3542 - accuracy: 0.8501\n",
      "Epoch 6/7\n",
      "48/48 [==============================] - 16s 329ms/step - loss: 0.3294 - accuracy: 0.8589\n",
      "Epoch 7/7\n",
      "48/48 [==============================] - 15s 322ms/step - loss: 0.3051 - accuracy: 0.8741\n",
      "Epoch 1/7\n",
      "48/48 [==============================] - 16s 323ms/step - loss: 0.5190 - accuracy: 0.7501\n",
      "Epoch 2/7\n",
      "48/48 [==============================] - 16s 324ms/step - loss: 0.4175 - accuracy: 0.8167\n",
      "Epoch 3/7\n",
      "48/48 [==============================] - 16s 326ms/step - loss: 0.3878 - accuracy: 0.8304\n",
      "Epoch 4/7\n",
      "48/48 [==============================] - 16s 328ms/step - loss: 0.3602 - accuracy: 0.8432\n",
      "Epoch 5/7\n",
      "48/48 [==============================] - 17s 362ms/step - loss: 0.3395 - accuracy: 0.8547\n",
      "Epoch 6/7\n",
      "48/48 [==============================] - 17s 349ms/step - loss: 0.3135 - accuracy: 0.8668\n",
      "Epoch 7/7\n",
      "48/48 [==============================] - 16s 343ms/step - loss: 0.2930 - accuracy: 0.8747\n",
      "Epoch 1/7\n",
      "48/48 [==============================] - 18s 367ms/step - loss: 0.5201 - accuracy: 0.7501\n",
      "Epoch 2/7\n",
      "48/48 [==============================] - 16s 339ms/step - loss: 0.4111 - accuracy: 0.8230\n",
      "Epoch 3/7\n",
      "48/48 [==============================] - 17s 352ms/step - loss: 0.3768 - accuracy: 0.8396\n",
      "Epoch 4/7\n",
      "48/48 [==============================] - 16s 326ms/step - loss: 0.3508 - accuracy: 0.8473\n",
      "Epoch 5/7\n",
      "48/48 [==============================] - 16s 327ms/step - loss: 0.3280 - accuracy: 0.8589\n",
      "Epoch 6/7\n",
      "48/48 [==============================] - 16s 327ms/step - loss: 0.3077 - accuracy: 0.8663\n",
      "Epoch 7/7\n",
      "48/48 [==============================] - 16s 326ms/step - loss: 0.2806 - accuracy: 0.8880\n",
      "Epoch 1/7\n",
      "48/48 [==============================] - 16s 337ms/step - loss: 0.5281 - accuracy: 0.7452\n",
      "Epoch 2/7\n",
      "48/48 [==============================] - 16s 323ms/step - loss: 0.4121 - accuracy: 0.8187\n",
      "Epoch 3/7\n",
      "48/48 [==============================] - 16s 326ms/step - loss: 0.3938 - accuracy: 0.8289\n",
      "Epoch 4/7\n",
      "48/48 [==============================] - 16s 323ms/step - loss: 0.3720 - accuracy: 0.8388\n",
      "Epoch 5/7\n",
      "48/48 [==============================] - 16s 324ms/step - loss: 0.3527 - accuracy: 0.8514\n",
      "Epoch 6/7\n",
      "48/48 [==============================] - 16s 324ms/step - loss: 0.3320 - accuracy: 0.8591\n",
      "Epoch 7/7\n",
      "48/48 [==============================] - 16s 324ms/step - loss: 0.3059 - accuracy: 0.8742\n",
      "Epoch 1/7\n",
      "48/48 [==============================] - 16s 324ms/step - loss: 0.5289 - accuracy: 0.7482\n",
      "Epoch 2/7\n",
      "48/48 [==============================] - 16s 324ms/step - loss: 0.4200 - accuracy: 0.8178\n",
      "Epoch 3/7\n",
      "48/48 [==============================] - 16s 327ms/step - loss: 0.3948 - accuracy: 0.8304\n",
      "Epoch 4/7\n",
      "48/48 [==============================] - 16s 325ms/step - loss: 0.3706 - accuracy: 0.8376\n",
      "Epoch 5/7\n",
      "48/48 [==============================] - 16s 325ms/step - loss: 0.3465 - accuracy: 0.8540\n",
      "Epoch 6/7\n",
      "48/48 [==============================] - 16s 328ms/step - loss: 0.3253 - accuracy: 0.8642\n",
      "Epoch 7/7\n",
      "48/48 [==============================] - 16s 328ms/step - loss: 0.2965 - accuracy: 0.8765\n",
      "Error de test: [0.7814593058416086, 0.7778783709411117, 0.759488270316677, 0.7718667127868096, 0.7924871082765819]\n"
     ]
    }
   ],
   "source": [
    "test_error=[]\n",
    "#Si agregamdp una dense mejora el resultado\n",
    "for train_index,test_index in kf.split(train_data):\n",
    "    x_train_words,x_test_words = train_data[train_index], train_data[test_index]\n",
    "    x_train_numerical,x_test_numerical = trainContinuous[train_index], trainContinuous[test_index]\n",
    "    y_train,y_test = train_df['target'][train_index], train_df['target'][test_index]\n",
    "    \n",
    "    emb = Embedding(50000, 200, input_length=100, weights=[embedding_matrix], trainable=False)(nlp_input)\n",
    "    drop = Dropout(0.2)(emb)#agregada propia (creo que andaba mejor antes de agregar esto)\n",
    "    conv = Conv1D(64, 5, activation='relu') (drop) #agregada propia (creo que andaba mejor antes de agregar esto)\n",
    "    mp = MaxPooling1D(pool_size=4)(conv)#agregada propia (creo que andaba mejor antes de agregar esto)\n",
    "    nlp_out = Bidirectional(LSTM(128))(emb)  #conv antes era emb\n",
    "    \n",
    "    x = concatenate([nlp_out, meta_input])\n",
    "    x = Dense(24, activation='relu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[nlp_input , meta_input], outputs=[x])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    model.fit([x_train_words, x_train_numerical], y_train, batch_size=128, epochs=7)\n",
    "    \n",
    "    predictions= model.predict([x_test_words,x_test_numerical])\n",
    "    \n",
    "    bin_pred = []\n",
    "    for i in predictions:\n",
    "        if (i<0.5):\n",
    "            bin_pred.append(0)\n",
    "        else:\n",
    "            bin_pred.append(1)\n",
    "\n",
    "    test_error.append(f1_score(y_test, bin_pred, average='macro'))\n",
    "    \n",
    "print('Error de test:',test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con dropout, conv1D, maxpooling y 3 epochs (estandar):\n",
    "Error de test: [0.7888178425057502, 0.7914766253113024, 0.7840449223167527, 0.746530115508615, 0.7930096559227526]\n",
    "    \n",
    "#SIN dropout,ni conv1D,ni maxpooling y con 3 epochs (da un poquito mejor sin nada de lo que le agregue):\n",
    "Error de test: [0.7907451626963822, 0.786468158944994, 0.7854915546911962, 0.7873662261730481, 0.8080302711793146]\n",
    "    \n",
    "#Con dropout, conv1D, maxpooling PERO con 5 epochs (mejoro un poquito tambien):\n",
    "Error de test: [0.805207329733444, 0.7851134723292488, 0.7747011121720304, 0.7838792991204754, 0.793572611501342]\n",
    "    \n",
    "#Con dropout, conv1D, maxpooling y 3 epochs PERO con vectores de 200dims:\n",
    "Error de test: [0.8013598421267523, 0.7831023324122048, 0.7737440449504573, 0.7866347906188544, 0.8026946909920577]\n",
    "\n",
    "#Mezcla de todo, ya que con todo lo mejoró(tarda una banda):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= model.predict([test_data,testContinuous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_pred = []\n",
    "for i in predictions:\n",
    "    if (i<0.5):\n",
    "        bin_pred.append(0)\n",
    "    else:\n",
    "        bin_pred.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  target\n",
       "0    0       1\n",
       "1    2       1\n",
       "2    3       1\n",
       "3    9       1\n",
       "4   11       1\n",
       "5   12       1\n",
       "6   21       0\n",
       "7   22       0\n",
       "8   27       0\n",
       "9   29       0\n",
       "10  30       0\n",
       "11  35       0\n",
       "12  42       0\n",
       "13  43       0\n",
       "14  45       0\n",
       "15  46       0\n",
       "16  47       0\n",
       "17  51       1\n",
       "18  58       0\n",
       "19  60       0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.target = bin_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('neural_network_doble_input_triple_epochs_more_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mismo pero meto conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index,test_index in kf.split(train_data):\n",
    "    x_train_words,x_test_words = train_data[train_index], train_data[test_index]\n",
    "    x_train_numerical,x_test_numerical = trainContinuous[train_index], trainContinuous[test_index]\n",
    "    y_train,y_test = train_df['target'][train_index], train_df['target'][test_index]\n",
    "    \n",
    "    emb = Embedding(50000, 200, input_length=100, weights=[embedding_matrix], trainable=False)(nlp_input)\n",
    "    drop = Dropout(0.2)(emb)#agregada propia (creo que andaba mejor antes de agregar esto)\n",
    "    conv = Conv1D(64, 5, activation='relu') (drop) #agregada propia (creo que andaba mejor antes de agregar esto)\n",
    "    mp = MaxPooling1D(pool_size=4)(conv)#agregada propia (creo que andaba mejor antes de agregar esto)\n",
    "    nlp_out = Bidirectional(LSTM(128))(emb)  #conv antes era emb\n",
    "    \n",
    "    x = concatenate([nlp_out, meta_input])\n",
    "    x = Dense(24, activation='relu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[nlp_input , meta_input], outputs=[x])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    model.fit([x_train_words, x_train_numerical], y_train, batch_size=128, epochs=5)\n",
    "    \n",
    "    predictions= model.predict([x_test_words,x_test_numerical])\n",
    "    \n",
    "    bin_pred = []\n",
    "    for i in predictions:\n",
    "        if (i<0.5):\n",
    "            bin_pred.append(0)\n",
    "        else:\n",
    "            bin_pred.append(1)\n",
    "\n",
    "    test_error.append(f1_score(y_test, bin_pred, average='macro'))\n",
    "    \n",
    "print('Error de test:',test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
